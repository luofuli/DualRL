# DualRL
[A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer](https://export.arxiv.org/pdf/1905.10060)
<p align="center"><img src="fig/poster.png"></p>

## Reproducibility
- Generated results of all baselines and our model are in the `outputs` directory.
- Human references are in the `references` directory. We also release the three more references we collected on the yelp test dataset, namely `reference[1,2,3].0`(the transferred references of negative sentences) and `reference[0,1,2,3].1` (the transferred references of positive sentences). The `reference0.0` and `reference0.1` are collected by [Li et al., 2018](https://github.com/shentianxiao/language-style-transfer). We **strongly recommend** that you use the released multi-references because it has a stronger correlation with human evaluation results.

## Dataset
### [Yelp](https://www.yelp.com/dataset/challenge): negative (0) <--> positive (1)
The yelp dataset is in the `data/yelp` directory. It consists of:
- The original train/dev/test data. Note that `x.0` denotes the negative `x` type of data and `x.1` denotes the positive `x` type of data. `x` is in `[train, dev, test]`.
- The pseudo-parallel data generated by templates can be found in the `data/yelp/tsf_template` directory. `x.0.tsf` denotes the negative transferred file in which each line only has the sentiment transferred sentence, while `x.0-1.tsf` denotes the negative transferred file in which each line has both the original sentence (input) and sentiment transferred sentence (output).

### [GYAFC](https://github.com/raosudha89/GYAFC-corpus): informal (0) <--> formal (1)

Since the GYAFC dataset is only free of charge for research purposes, we only publish a subset of the test dataset in the family and relationships domain and the corresponding outputs of each system (including our model and all baselines). If you want to download the train and validation dataset, please follow the guidance at [https://github.com/raosudha89/GYAFC-corpus](https://github.com/raosudha89/GYAFC-corpus). And then, name the corpora of two styles as the yelp dataset.


## Quick Start
First of all, you should specify the dataset at 9-10 line in the `common_options.py`.
Then, run the following commands.

### Step 1: Pre-train classifier
```
cd classifier
python textcnn.py --mode train
```

### Step 2: Pre-train two seq2seq (nmt) models using pseudo-parallel data
#### Prepare pseudo-parallel data 
To generate pseudo-parallel data, we follow the [template-based method](https://github.com/shentianxiao/language-style-transfer) proposed by [Li et al., 2018](https://aclweb.org/anthology/N18-1169). And we have provided the pseudo-parallel data of the yelp dataset in the `data/yelp/tsf_template` directory. However, if you want to generate the pseudo-parallel data using templates, you can follow [this link](https://github.com/lijuncen/Sentiment-and-Style-Transfer/issues/6) or design your own templates which are suitable for your task and dataset.

#### Pre-train two seq2seq (nmt) models 
The default encoder and decoder are **bilstm**.
```
cd nmt
python nmt.py --mode train --nmt_direction 0-1 --n_epoch 5  # Pre-train forward (f) model
python nmt.py --mode train --nmt_direction 1-0 --n_epoch 5  # Pre-train backward (g) model
```

If you want to adopt **transformer** as encoder and decoder, run the following code:
```
cd nmt
python nmt.py --mode train --nmt_direction 1-0 --n_epoch 5 --n_layer 6 --encoder_decoder_type transformer
python nmt.py --mode train --nmt_direction 1-0 --n_epoch 5 --n_layer 6 --encoder_decoder_type transformer
```

### Step 3: Dual reinforcement learning
```
python dual_training.py --n_epoch 10
```

## Extend to other tasks and datasets
If you don't have parallel or paired data, here are the processes you might go through:
1. Prepare two unaligned (unpaired) corpora, one sentence per line
2. Divide the dataset into train/dev/test
3. Prepare pseudo-parallel corpus, you can use [Li's](https://github.com/lijuncen/Sentiment-and-Style-Transfer/issues/6) method, or your own designed heuristic rules/templates
4. Run step 1-3 in the section of Quick Start and specify the path to your new dataset (you can run the following code to see which parameters need to be set)
```
python [dual_training.py | nmt.py | textcnn.py] --help
```

If you have parallel or paired data, here are the processes you might go through:
1. Prepare two parallel corpora, one sentence per line
2. Divide the dataset into train/dev/test
3. Copy the dataset generated in the second step as the "pseudo-parallel" corpus
4. Run step 1-3 in the section of Quick Start and specify the path to your new dataset

## Dependencies
```
python==2.7
numpy==1.14.2
tensorflow==1.13.1
OpenNMT_tf==1.15.0
```

## Cite

If you use this code, please cite the following paper:
```
@inproceedings{Luo19DualRL,
  author    = {Fuli Luo and
               Peng Li and
               Jie Zhou and
               Pengcheng Yang and
               Baobao Chang and
               Zhifang Sui and
               Xu Sun},
  title     = {A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer},
  booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence, {IJCAI} 2019},
  year      = {2019},            
}
```
